"""
EMç®—æ³•æ¨¡å— - è´å¶æ–¯ç½‘ç»œå‚æ•°å­¦ä¹  (è°ƒè¯•ç‰ˆæœ¬)

ä¿®å¤äº†å¯¹æ•°ä¼¼ç„¶è®¡ç®—é—®é¢˜ï¼Œæ·»åŠ äº†è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
"""

import numpy as np
import pandas as pd
import pickle
import time
from typing import Tuple, List, Dict, Any

def em_algorithm(bn, df, mis_idx, max_iter=50, tolerance=1e-4, verbose=True):
    """
    EMç®—æ³•ä¸»å‡½æ•° - è°ƒè¯•ç‰ˆæœ¬
    """
    if verbose:
        print("ğŸ”„ EMç®—æ³•åˆå§‹åŒ–...")
        print(f"ğŸ“Š å‚æ•°è®¾ç½®: æœ€å¤§è¿­ä»£={max_iter}, å®¹å·®={tolerance}")
        
        # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ•°æ®åŸºæœ¬æƒ…å†µ
        print("\nğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"   ç½‘ç»œèŠ‚ç‚¹æ•°: {len(bn.Pres_Graph)}")
        print(f"   å®Œæ•´è®°å½•æ•°: {sum(1 for x in mis_idx if x == -1)}")
        print(f"   ç¼ºå¤±è®°å½•æ•°: {sum(1 for x in mis_idx if x != -1)}")
        
        # æ£€æŸ¥æ•°æ®æ ·æœ¬
        print(f"\nğŸ” æ•°æ®æ ·æœ¬:")
        print(df.head(3))
        print(f"\nğŸ” æ•°æ®ç±»å‹:")
        print(df.dtypes.head(5))
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„CPT
        first_node_name = list(bn.Pres_Graph.keys())[0]
        first_node = bn.Pres_Graph[first_node_name]
        print(f"\nğŸ” èŠ‚ç‚¹ '{first_node_name}' CPTæ ·æœ¬:")
        print(first_node.cpt_data.head())
        print(f"   CPTå½¢çŠ¶: {first_node.cpt_data.shape}")
        print(f"   æ˜¯å¦æœ‰'p'åˆ—: {'p' in first_node.cpt_data.columns}")
        
    log_likelihood_history = []
    log_likelihood_old = -np.inf
    
    for iteration in range(max_iter):
        if verbose:
            print(f"\nğŸ”„ è¿­ä»£ {iteration + 1}/{max_iter}")
        
        iter_start = time.time()
        
        # Eæ­¥ï¼šè®¡ç®—æœŸæœ›å……åˆ†ç»Ÿè®¡é‡
        e_start = time.time()
        e_step_debug(bn, df, mis_idx, verbose and iteration == 0)
        e_time = time.time() - e_start
        
        # Mæ­¥ï¼šæ›´æ–°å‚æ•°
        m_start = time.time()
        m_step_debug(bn, verbose and iteration == 0)
        m_time = time.time() - m_start
        
        # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
        likelihood_start = time.time()
        log_likelihood = compute_log_likelihood_debug(bn, df, mis_idx, verbose and iteration == 0)
        likelihood_time = time.time() - likelihood_start
        
        log_likelihood_history.append(log_likelihood)
        
        if verbose:
            print(f"   â±ï¸  Eæ­¥: {e_time:.2f}s | Mæ­¥: {m_time:.2f}s | ä¼¼ç„¶: {likelihood_time:.2f}s")
            print(f"   ğŸ“ˆ å¯¹æ•°ä¼¼ç„¶: {log_likelihood:.6f}")
            
            if iteration > 0:
                improvement = log_likelihood - log_likelihood_old
                print(f"   ğŸ“Š æ”¹è¿›: {improvement:.8f}")
        
        # æ£€æŸ¥æ”¶æ•›
        if iteration > 0 and abs(log_likelihood - log_likelihood_old) < tolerance:
            if verbose:
                print(f"âœ… ç®—æ³•æ”¶æ•›ï¼è¿­ä»£ {iteration + 1} æ¬¡")
            break
            
        log_likelihood_old = log_likelihood
    else:
        if verbose:
            print(f"âš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iter}")
    
    if verbose:
        print_convergence_summary(log_likelihood_history)
    
    return bn, log_likelihood

def e_step_debug(bn, df, mis_idx, debug=False):
    """Eæ­¥ - è°ƒè¯•ç‰ˆæœ¬"""
    if debug:
        print("\nğŸ” Eæ­¥è°ƒè¯•:")
    
    # é‡ç½®æ‰€æœ‰èŠ‚ç‚¹çš„è®¡æ•°
    reset_all_counts(bn)
    
    complete_count = 0
    missing_count = 0
    
    # å¤„ç†æ¯æ¡è®°å½•
    for idx, (_, record) in enumerate(df.iterrows()):
        if mis_idx[idx] == -1:
            # å®Œæ•´è®°å½•ï¼šç›´æ¥ç´¯åŠ è®¡æ•°
            process_complete_record_debug(bn, record, debug and idx < 3)
            complete_count += 1
        else:
            # ç¼ºå¤±è®°å½•ï¼šè®¡ç®—æœŸæœ›è®¡æ•°
            process_missing_record_debug(bn, record, mis_idx[idx], df.columns, debug and idx < 3)
            missing_count += 1
    
    if debug:
        print(f"   å¤„ç†å®Œæ•´è®°å½•: {complete_count}")
        print(f"   å¤„ç†ç¼ºå¤±è®°å½•: {missing_count}")
        
        # æ£€æŸ¥è®¡æ•°æ›´æ–°æƒ…å†µ
        first_node = list(bn.Pres_Graph.values())[0]
        total_counts = first_node.cpt_data['counts'].sum()
        print(f"   ç¬¬ä¸€ä¸ªèŠ‚ç‚¹æ€»è®¡æ•°: {total_counts}")

def m_step_debug(bn, debug=False):
    """Mæ­¥ - è°ƒè¯•ç‰ˆæœ¬"""
    if debug:
        print("\nğŸ” Mæ­¥è°ƒè¯•:")
        
        # æ£€æŸ¥å½’ä¸€åŒ–å‰çš„è®¡æ•°
        first_node = list(bn.Pres_Graph.values())[0]
        print(f"   å½’ä¸€åŒ–å‰è®¡æ•°èŒƒå›´: {first_node.cpt_data['counts'].min():.4f} - {first_node.cpt_data['counts'].max():.4f}")
    
    for node_name in bn.Pres_Graph:
        bn.normalise_cpt(node_name)
    
    if debug:
        # æ£€æŸ¥å½’ä¸€åŒ–åçš„æ¦‚ç‡
        first_node = list(bn.Pres_Graph.values())[0]
        if 'p' in first_node.cpt_data.columns:
            print(f"   å½’ä¸€åŒ–åæ¦‚ç‡èŒƒå›´: {first_node.cpt_data['p'].min():.4f} - {first_node.cpt_data['p'].max():.4f}")
            print(f"   æ¦‚ç‡æ€»å’Œæ£€æŸ¥: {first_node.cpt_data['p'].sum():.4f}")

def process_complete_record_debug(bn, record, debug=False):
    """å¤„ç†å®Œæ•´è®°å½• - è°ƒè¯•ç‰ˆæœ¬"""
    if debug:
        print(f"\nğŸ” å¤„ç†å®Œæ•´è®°å½•è°ƒè¯•:")
        print(f"   è®°å½•å†…å®¹: {dict(record.head())}")
    
    updates_made = 0
    
    for node_name, node in bn.Pres_Graph.items():
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = build_query_conditions_debug(node, record, debug and updates_made < 2)
        
        # æ›´æ–°åŒ¹é…è¡Œçš„è®¡æ•°
        updated = update_node_counts_debug(node, conditions, 1.0, debug and updates_made < 2)
        if updated:
            updates_made += 1
    
    if debug:
        print(f"   æˆåŠŸæ›´æ–°èŠ‚ç‚¹æ•°: {updates_made}")

def build_query_conditions_debug(node, record, debug=False):
    """æ„å»ºæŸ¥è¯¢æ¡ä»¶ - è°ƒè¯•ç‰ˆæœ¬"""
    conditions = {}
    
    # æ·»åŠ çˆ¶èŠ‚ç‚¹å€¼
    for parent in node.Parents:
        if pd.notna(record[parent]):
            conditions[parent] = record[parent]
    
    # æ·»åŠ å½“å‰èŠ‚ç‚¹å€¼
    if pd.notna(record[node.Node_Name]):
        conditions[node.Node_Name] = record[node.Node_Name]
    
    if debug:
        print(f"     èŠ‚ç‚¹ {node.Node_Name}: æ¡ä»¶ = {conditions}")
    
    return conditions

def update_node_counts_debug(node, conditions, weight, debug=False):
    """æ›´æ–°èŠ‚ç‚¹è®¡æ•° - è°ƒè¯•ç‰ˆæœ¬"""
    if not conditions:
        if debug:
            print(f"     è·³è¿‡: æ— æœ‰æ•ˆæ¡ä»¶")
        return False
    
    # æ„å»ºåŒ¹é…æ©ç 
    mask = pd.Series([True] * len(node.cpt_data))
    for col, val in conditions.items():
        if col in node.cpt_data.columns:
            mask = mask & (node.cpt_data[col] == val)
        else:
            if debug:
                print(f"     è­¦å‘Š: åˆ— '{col}' ä¸åœ¨CPTä¸­")
            return False
    
    # æ›´æ–°åŒ¹é…è¡Œçš„è®¡æ•°
    matched_count = mask.sum()
    if matched_count > 0:
        old_count = node.cpt_data.loc[mask, 'counts'].iloc[0] if matched_count > 0 else 0
        node.cpt_data.loc[mask, 'counts'] += weight
        new_count = node.cpt_data.loc[mask, 'counts'].iloc[0] if matched_count > 0 else 0
        
        if debug:
            print(f"     åŒ¹é…è¡Œæ•°: {matched_count}, è®¡æ•°: {old_count:.2f} -> {new_count:.2f}")
        return True
    else:
        if debug:
            print(f"     è­¦å‘Š: æ— åŒ¹é…è¡Œ")
        return False

def process_missing_record_debug(bn, record, missing_var_idx, columns, debug=False):
    """å¤„ç†ç¼ºå¤±è®°å½• - è°ƒè¯•ç‰ˆæœ¬"""
    missing_var = columns[missing_var_idx]
    missing_node = bn.search_node(missing_var)
    
    if missing_node is None:
        if debug:
            print(f"     è­¦å‘Š: æ‰¾ä¸åˆ°ç¼ºå¤±å˜é‡èŠ‚ç‚¹ '{missing_var}'")
        return
    
    if debug:
        print(f"\nğŸ” å¤„ç†ç¼ºå¤±è®°å½•: å˜é‡ '{missing_var}'")
    
    # è®¡ç®—æ¯ä¸ªå¯èƒ½å€¼çš„åéªŒæ¦‚ç‡
    probabilities = compute_posterior_probabilities_debug(bn, record, missing_var, missing_node, debug)
    
    if debug:
        print(f"   åéªŒæ¦‚ç‡: {probabilities}")
    
    # æŒ‰æ¦‚ç‡æƒé‡æ›´æ–°è®¡æ•°
    for val, prob in enumerate(probabilities):
        if prob > 1e-10:
            complete_record = record.copy()
            complete_record[missing_var] = val
            
            # ä¸ºæ‰€æœ‰èŠ‚ç‚¹æ›´æ–°åŠ æƒè®¡æ•°
            for node_name, node in bn.Pres_Graph.items():
                conditions = build_query_conditions_debug(node, complete_record, False)
                update_node_counts_debug(node, conditions, prob, False)

def compute_posterior_probabilities_debug(bn, record, missing_var, missing_node, debug=False):
    """è®¡ç®—åéªŒæ¦‚ç‡ - è°ƒè¯•ç‰ˆæœ¬"""
    probabilities = []
    
    for val in range(missing_node.nvalues):
        prob = compute_conditional_probability_debug(missing_node, record, missing_var, val, debug and val < 2)
        probabilities.append(prob)
    
    # å½’ä¸€åŒ–
    total_prob = sum(probabilities)
    if total_prob > 0:
        probabilities = [p / total_prob for p in probabilities]
    else:
        probabilities = [1.0 / missing_node.nvalues] * missing_node.nvalues
        if debug:
            print(f"   è­¦å‘Š: æ‰€æœ‰æ¦‚ç‡ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ")
    
    return probabilities

def compute_conditional_probability_debug(node, record, var_name, var_value, debug=False):
    """è®¡ç®—æ¡ä»¶æ¦‚ç‡ - è°ƒè¯•ç‰ˆæœ¬"""
    conditions = {}
    
    # æ·»åŠ çˆ¶èŠ‚ç‚¹æ¡ä»¶
    for parent in node.Parents:
        if pd.notna(record[parent]):
            conditions[parent] = record[parent]
    
    # æ·»åŠ å½“å‰å˜é‡å€¼
    conditions[var_name] = var_value
    
    if debug:
        print(f"     æŸ¥è¯¢æ¡ä»¶: {conditions}")
    
    # åœ¨CPTä¸­æŸ¥æ‰¾æ¦‚ç‡
    mask = pd.Series([True] * len(node.cpt_data))
    for col, val in conditions.items():
        if col in node.cpt_data.columns:
            mask = mask & (node.cpt_data[col] == val)
    
    matched_rows = node.cpt_data[mask]
    if len(matched_rows) > 0 and 'p' in matched_rows.columns:
        prob = matched_rows['p'].iloc[0]
        if debug:
            print(f"     æ‰¾åˆ°æ¦‚ç‡: {prob:.6f}")
        return prob
    else:
        prob = 1.0 / node.nvalues
        if debug:
            print(f"     æœªæ‰¾åˆ°ï¼Œä½¿ç”¨å‡åŒ€å…ˆéªŒ: {prob:.6f}")
        return prob

def compute_log_likelihood_debug(bn, df, mis_idx, debug=False):
    """è®¡ç®—å¯¹æ•°ä¼¼ç„¶ - è°ƒè¯•ç‰ˆæœ¬"""
    if debug:
        print("\nğŸ” å¯¹æ•°ä¼¼ç„¶è®¡ç®—è°ƒè¯•:")
    
    log_likelihood = 0.0
    complete_processed = 0
    missing_processed = 0
    zero_likelihood_count = 0
    
    for idx, (_, record) in enumerate(df.iterrows()):
        if mis_idx[idx] == -1:  # å®Œæ•´è®°å½•
            record_likelihood = compute_record_likelihood_debug(bn, record, debug and complete_processed < 3)
            
            if record_likelihood > 0:
                log_likelihood += np.log(record_likelihood)
            else:
                zero_likelihood_count += 1
                log_likelihood += np.log(1e-10)  # é¿å…log(0)
            
            complete_processed += 1
            
        else:
            # ç¼ºå¤±è®°å½•çš„è¾¹é™…ä¼¼ç„¶
            marginal_likelihood = compute_marginal_likelihood_debug(bn, record, mis_idx[idx], df.columns, debug and missing_processed < 2)
            
            if marginal_likelihood > 0:
                log_likelihood += np.log(marginal_likelihood)
            else:
                log_likelihood += np.log(1e-10)
            
            missing_processed += 1
    
    if debug:
        print(f"   å¤„ç†å®Œæ•´è®°å½•: {complete_processed}")
        print(f"   å¤„ç†ç¼ºå¤±è®°å½•: {missing_processed}")
        print(f"   é›¶ä¼¼ç„¶è®°å½•æ•°: {zero_likelihood_count}")
        print(f"   å¯¹æ•°ä¼¼ç„¶æ€»å’Œ: {log_likelihood:.6f}")
        
        if zero_likelihood_count > 0:
            print(f"   âš ï¸  æœ‰ {zero_likelihood_count} æ¡è®°å½•ä¼¼ç„¶ä¸º0ï¼")
    
    return log_likelihood

def compute_record_likelihood_debug(bn, record, debug=False):
    """è®¡ç®—è®°å½•ä¼¼ç„¶ - è°ƒè¯•ç‰ˆæœ¬"""
    likelihood = 1.0
    zero_prob_nodes = []
    
    if debug:
        print(f"\nğŸ” è®¡ç®—è®°å½•ä¼¼ç„¶:")
    
    for node_name, node in bn.Pres_Graph.items():
        conditions = build_query_conditions_debug(node, record, False)
        
        if len(conditions) == len(node.Parents) + 1:  # æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³
            prob = get_probability_from_cpt_debug(node, conditions, debug and len(zero_prob_nodes) < 3)
            if prob > 0:
                likelihood *= prob
            else:
                zero_prob_nodes.append(node_name)
                likelihood *= 1e-10
    
    if debug:
        print(f"   æœ€ç»ˆä¼¼ç„¶: {likelihood:.10f}")
        if zero_prob_nodes:
            print(f"   é›¶æ¦‚ç‡èŠ‚ç‚¹: {zero_prob_nodes[:3]}")
    
    return likelihood

def get_probability_from_cpt_debug(node, conditions, debug=False):
    """ä»CPTè·å–æ¦‚ç‡ - è°ƒè¯•ç‰ˆæœ¬"""
    mask = pd.Series([True] * len(node.cpt_data))
    for col, val in conditions.items():
        if col in node.cpt_data.columns:
            mask = mask & (node.cpt_data[col] == val)
    
    matched_rows = node.cpt_data[mask]
    if len(matched_rows) > 0 and 'p' in matched_rows.columns:
        prob = matched_rows['p'].iloc[0]
        if debug:
            print(f"   èŠ‚ç‚¹ {node.Node_Name}: æ¡ä»¶={conditions}, æ¦‚ç‡={prob:.6f}")
        return prob
    else:
        prob = 1.0 / node.nvalues
        if debug:
            print(f"   èŠ‚ç‚¹ {node.Node_Name}: æœªæ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨ {prob:.6f}")
        return prob

def compute_marginal_likelihood_debug(bn, record, missing_var_idx, columns, debug=False):
    """è®¡ç®—è¾¹é™…ä¼¼ç„¶ - è°ƒè¯•ç‰ˆæœ¬"""
    missing_var = columns[missing_var_idx]
    missing_node = bn.search_node(missing_var)
    
    if missing_node is None:
        return 1e-5
    
    marginal_prob = 0.0
    
    if debug:
        print(f"\nğŸ” è®¡ç®—è¾¹é™…ä¼¼ç„¶: å˜é‡ '{missing_var}'")
    
    # å¯¹ç¼ºå¤±å˜é‡çš„æ¯ä¸ªå¯èƒ½å€¼æ±‚å’Œ
    for val in range(missing_node.nvalues):
        complete_record = record.copy()
        complete_record[missing_var] = val
        
        record_prob = compute_record_likelihood_debug(bn, complete_record, False)
        marginal_prob += record_prob
        
        if debug and val < 2:
            print(f"   å€¼ {val}: æ¦‚ç‡ = {record_prob:.8f}")
    
    if debug:
        print(f"   è¾¹é™…æ¦‚ç‡: {marginal_prob:.8f}")
    
    return marginal_prob

def reset_all_counts(bn):
    """é‡ç½®æ‰€æœ‰èŠ‚ç‚¹çš„è®¡æ•°ä¸º0"""
    for node in bn.Pres_Graph.values():
        node.cpt_data['counts'] = 0.0

def print_convergence_summary(log_likelihood_history):
    """æ‰“å°æ”¶æ•›æ‘˜è¦"""
    print("\nğŸ“ˆ æ”¶æ•›æ‘˜è¦:")
    print(f"   åˆå§‹å¯¹æ•°ä¼¼ç„¶: {log_likelihood_history[0]:.6f}")
    print(f"   æœ€ç»ˆå¯¹æ•°ä¼¼ç„¶: {log_likelihood_history[-1]:.6f}")
    if len(log_likelihood_history) > 1:
        total_improvement = log_likelihood_history[-1] - log_likelihood_history[0]
        print(f"   æ€»æ”¹è¿›: {total_improvement:.6f}")

def save_trained_network(bn, filename):
    """ä¿å­˜è®­ç»ƒåçš„ç½‘ç»œ"""
    try:
        with open(filename, 'wb') as f:
            pickle.dump(bn, f)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def load_trained_network(filename):
    """åŠ è½½è®­ç»ƒåçš„ç½‘ç»œ"""
    try:
        with open(filename, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None