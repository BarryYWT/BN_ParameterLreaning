
from __future__ import division
import numpy as np
import pandas as pd
import bayesnet

# ---------------  Data helpers  -------------------------------------------- #
# def get_data(filepath: str) -> pd.DataFrame:
#     """Load records.dat & convert categorical strings to integers."""
#     '''
#     æ•°æ®é¢„å¤„ç†:
#     åŠ è½½: è¯»å– records.dat æ–‡ä»¶ï¼ˆåŸå§‹æ•°æ®ï¼‰
#     åˆ—åè®¾ç½®: 37ä¸ªå˜é‡çš„æ ‡å‡†åˆ—å
#     æ•°æ®æ˜ å°„: å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—ç¼–ç 
#     '''
#     with open(filepath, 'r', encoding='utf-8') as f:
#         raw = [line.rstrip().split() for line in f]
#     df = pd.DataFrame(raw)

#     cols = ["Hypovolemia","StrokeVolume","LVFailure","LVEDVolume","PCWP","CVP","History",
#             "MinVolSet","VentMach","Disconnect","VentTube","KinkedTube","Press","ErrLowOutput",
#             "HRBP","ErrCauter","HREKG","HRSat","BP","CO","HR","TPR","Anaphylaxis","InsuffAnesth",
#             "PAP","PulmEmbolus","FiO2","Catechol","SaO2","Shunt","PVSat","MinVol","ExpCO2",
#             "ArtCO2","VentAlv","VentLung","Intubation"]
#     df.columns = cols

#     def _map(d):
#         return {k: i for i, k in enumerate(d)}

#     mappings = {
#         **{c: {"True":0,"False":1,"?":np.nan} for c in
#            ["Hypovolemia","LVFailure","History","Disconnect","KinkedTube",
#             "ErrLowOutput","ErrCauter","Anaphylaxis","InsuffAnesth","PulmEmbolus"]},
#         **{c: {"Zero":0,"Low":1,"Normal":2,"High":3,"?":np.nan} for c in
#            ["VentMach","VentTube","Press","MinVol","ExpCO2","VentAlv","VentLung"]},
#         **{c: {"Low":0,"Normal":1,"High":2,"?":np.nan} for c in
#            ["StrokeVolume","LVEDVolume","PCWP","CVP","HRBP","HREKG",
#             "HRSat","BP","CO","HR","TPR","PAP","SaO2","PVSat","ArtCO2"]},
#         "FiO2": {"Low":0,"Normal":1,"?":np.nan},
#         "Catechol": {"Normal":0,"High":1,"?":np.nan},
#         "Shunt": {"Normal":0,"High":1,"?":np.nan},
#         "Intubation": {"Normal":0,"Esophageal":1,"OneSided":2,"?":np.nan},
#     }

#     df.replace(mappings, inplace=True)
#     return df.apply(pd.to_numeric, errors='coerce')
def get_data(filepath: str) -> pd.DataFrame:
    '''
    æ•°æ®é¢„å¤„ç†: ä¿®å¤ç‰ˆæœ¬
    åŠ è½½: è¯»å– records.dat æ–‡ä»¶ï¼ˆåŸå§‹æ•°æ®ï¼‰
    åˆ—åè®¾ç½®: 37ä¸ªå˜é‡çš„æ ‡å‡†åˆ—å
    æ•°æ®æ˜ å°„: å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—ç¼–ç 
    '''
    print("ğŸ” è°ƒè¯• get_data():")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        lines = [line.rstrip() for line in f]
    
    print(f"   æ–‡ä»¶è¡Œæ•°: {len(lines)}")
    print(f"   ç¬¬ä¸€è¡ŒåŸå§‹: {lines[0][:100]}...")
    
    # ä¿®å¤: å…ˆåˆ†å‰²ï¼Œç„¶åå»é™¤åŒå¼•å·
    raw = []
    for line in lines:
        # åˆ†å‰²å¹¶å»é™¤åŒå¼•å·
        fields = [field.strip('"') for field in line.split()]
        raw.append(fields)
    
    df = pd.DataFrame(raw)
    print(f"   DataFrameå½¢çŠ¶: {df.shape}")
    print(f"   ç¬¬ä¸€è¡Œå¤„ç†å: {raw[0][:5]}")
    
    # è®¾ç½®åˆ—å
    cols = ["Hypovolemia","StrokeVolume","LVFailure","LVEDVolume","PCWP","CVP","History",
            "MinVolSet","VentMach","Disconnect","VentTube","KinkedTube","Press","ErrLowOutput",
            "HRBP","ErrCauter","HREKG","HRSat","BP","CO","HR","TPR","Anaphylaxis","InsuffAnesth",
            "PAP","PulmEmbolus","FiO2","Catechol","SaO2","Shunt","PVSat","MinVol","ExpCO2",
            "ArtCO2","VentAlv","VentLung","Intubation"]
    df.columns = cols

    # æ£€æŸ¥å”¯ä¸€å€¼
    print(f"   å‰3åˆ—çš„å”¯ä¸€å€¼:")
    for col in cols[:3]:
        unique_vals = df[col].unique()[:5]
        print(f"     {col}: {unique_vals}")

    # æ•°æ®æ˜ å°„
    mappings = {
        **{c: {"True":0,"False":1,"?":np.nan} for c in
           ["Hypovolemia","LVFailure","History","Disconnect","KinkedTube",
            "ErrLowOutput","ErrCauter","Anaphylaxis","InsuffAnesth","PulmEmbolus"]},
        **{c: {"Zero":0,"Low":1,"Normal":2,"High":3,"?":np.nan} for c in
           ["VentMach","VentTube","Press","MinVol","ExpCO2","VentAlv","VentLung"]},
        **{c: {"Low":0,"Normal":1,"High":2,"?":np.nan} for c in
           ["StrokeVolume","LVEDVolume","PCWP","CVP","HRBP","HREKG",
            "HRSat","BP","CO","HR","TPR","PAP","SaO2","PVSat","ArtCO2"]},
        "FiO2": {"Low":0,"Normal":1,"?":np.nan},
        "Catechol": {"Normal":0,"High":1,"?":np.nan},
        "Shunt": {"Normal":0,"High":1,"?":np.nan},
        "Intubation": {"Normal":0,"Esophageal":1,"OneSided":2,"?":np.nan},
    }

    df.replace(mappings, inplace=True)
    
    # æ£€æŸ¥æ˜ å°„åçš„æƒ…å†µ
    print(f"   æ˜ å°„åå‰3åˆ—å”¯ä¸€å€¼:")
    for col in cols[:3]:
        unique_vals = df[col].unique()[:5]
        print(f"     {col}: {unique_vals}")
    
    result = df.apply(pd.to_numeric, errors='coerce')
    
    # æ£€æŸ¥æœ€ç»ˆç»“æœ
    nan_count = result.isna().sum().sum()
    total_values = result.size
    print(f"   NaNæ•°é‡: {nan_count}/{total_values}")
    print(f"   å®Œæ•´è®°å½•æ•°: {(~result.isna().any(axis=1)).sum()}")
    
    return result

# ---------------  Network initialisation  ---------------------------------- #
def setup_network(bif_alarm: str, dat_records: str):
    '''
    åŠŸèƒ½: ä¸€ç«™å¼å»ºç«‹å®Œæ•´çš„è´å¶æ–¯ç½‘ç»œç³»ç»Ÿ
        è¯»å–ç½‘ç»œç»“æ„: ä» .bif æ–‡ä»¶æ„å»ºç½‘ç»œ
        è®¾ç½®é©¬å°”å¯å¤«æ¯¯: é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„é©¬å°”å¯å¤«æ¯¯
        åŠ è½½æ•°æ®: å¤„ç†è§‚æµ‹æ•°æ®
        å‚æ•°åˆå§‹åŒ–: ä¸ºEMç®—æ³•å‡†å¤‡åˆå§‹å‚æ•°
        ç¼ºå¤±æ•°æ®ç´¢å¼•: è®°å½•æ¯è¡Œç¼ºå¤±å˜é‡çš„ä½ç½®
    '''
    print("0: Reading Network . . . ")
    Alarm = bayesnet.read_network(bif_alarm)

    print("1: Setting Markov Blankets . . . ")
    Alarm.set_mb()

    print("2: Getting data from records . . . ")
    df = get_data(dat_records)

    print("3: Initialising parameters . . . ")
    init_params(df, Alarm)

    print("4: Getting missing data indexes . . . ")
    mis_index = get_missing_index(df)

    return Alarm, df, mis_index

# ---------------  EM initialisation helpers  ------------------------------- #
def get_missing_index(df):
    '''
    #ç¼ºå¤±æ•°æ®å®šä½
    ## è¿”å›æ¯è¡Œç¬¬ä¸€ä¸ªç¼ºå¤±å˜é‡çš„ç´¢å¼•ï¼Œå®Œæ•´è¡Œè¿”å›-1
    '''
    idx = []
    for _, row in df.iterrows():
        if row.isnull().any():
            idx.append(int(np.where(row.isna())[0][0]))
        else:
            idx.append(-1)
    return idx

def init_params(df, net):
    for node in net.Pres_Graph.values():
        # simple uniform prior until EM runs
        node.cpt_data["counts"] = 1.0
        net.normalise_cpt(node.Node_Name)
