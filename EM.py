"""
EMç®—æ³•æ¨¡å— - è´å¶æ–¯ç½‘ç»œå‚æ•°å­¦ä¹ 

ä¸»è¦åŠŸèƒ½ï¼š
1. EMç®—æ³•ä¸»æµç¨‹
2. Eæ­¥ï¼šæœŸæœ›è®¡ç®—
3. Mæ­¥ï¼šå‚æ•°æ›´æ–°
4. æ”¶æ•›æ£€æµ‹
5. è¾…åŠ©å·¥å…·å‡½æ•°
"""

import numpy as np
import pandas as pd
import pickle
import time
from typing import Tuple, List, Dict, Any

def em_algorithm(bn, df, mis_idx, max_iter=50, tolerance=1e-4, verbose=True):
    """
    EMç®—æ³•ä¸»å‡½æ•°
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œå¯¹è±¡
        df: è§‚æµ‹æ•°æ®DataFrame
        mis_idx: ç¼ºå¤±æ•°æ®ç´¢å¼•æ•°ç»„
        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        tolerance: æ”¶æ•›å®¹å·®
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        tuple: (è®­ç»ƒåçš„ç½‘ç»œ, æœ€ç»ˆå¯¹æ•°ä¼¼ç„¶)
    """
    if verbose:
        print("ğŸ”„ EMç®—æ³•åˆå§‹åŒ–...")
        print(f"ğŸ“Š å‚æ•°è®¾ç½®: æœ€å¤§è¿­ä»£={max_iter}, å®¹å·®={tolerance}")
    
    log_likelihood_history = []
    log_likelihood_old = -np.inf
    
    for iteration in range(max_iter):
        if verbose:
            print(f"\nğŸ”„ è¿­ä»£ {iteration + 1}/{max_iter}")
        
        iter_start = time.time()
        
        # Eæ­¥ï¼šè®¡ç®—æœŸæœ›å……åˆ†ç»Ÿè®¡é‡
        e_start = time.time()
        e_step(bn, df, mis_idx)
        e_time = time.time() - e_start
        
        # Mæ­¥ï¼šæ›´æ–°å‚æ•°
        m_start = time.time()
        m_step(bn)
        m_time = time.time() - m_start
        
        # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
        likelihood_start = time.time()
        log_likelihood = compute_log_likelihood(bn, df, mis_idx)
        likelihood_time = time.time() - likelihood_start
        
        log_likelihood_history.append(log_likelihood)
        
        iter_time = time.time() - iter_start
        
        if verbose:
            print(f"   â±ï¸  Eæ­¥: {e_time:.2f}s | Mæ­¥: {m_time:.2f}s | ä¼¼ç„¶: {likelihood_time:.2f}s")
            print(f"   ğŸ“ˆ å¯¹æ•°ä¼¼ç„¶: {log_likelihood:.6f}")
            
            if iteration > 0:
                improvement = log_likelihood - log_likelihood_old
                print(f"   ğŸ“Š æ”¹è¿›: {improvement:.8f}")
        
        # æ£€æŸ¥æ”¶æ•›
        if iteration > 0 and abs(log_likelihood - log_likelihood_old) < tolerance:
            if verbose:
                print(f"âœ… ç®—æ³•æ”¶æ•›ï¼è¿­ä»£ {iteration + 1} æ¬¡")
            break
            
        log_likelihood_old = log_likelihood
    else:
        if verbose:
            print(f"âš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iter}")
    
    if verbose:
        print_convergence_summary(log_likelihood_history)
    
    return bn, log_likelihood

def e_step(bn, df, mis_idx):
    """
    Eæ­¥ï¼šè®¡ç®—æœŸæœ›å……åˆ†ç»Ÿè®¡é‡
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œ
        df: è§‚æµ‹æ•°æ®
        mis_idx: ç¼ºå¤±æ•°æ®ç´¢å¼•
    """
    # é‡ç½®æ‰€æœ‰èŠ‚ç‚¹çš„è®¡æ•°
    reset_all_counts(bn)
    
    # å¤„ç†æ¯æ¡è®°å½•
    for idx, (_, record) in enumerate(df.iterrows()):
        if mis_idx[idx] == -1:
            # å®Œæ•´è®°å½•ï¼šç›´æ¥ç´¯åŠ è®¡æ•°
            process_complete_record(bn, record)
        else:
            # ç¼ºå¤±è®°å½•ï¼šè®¡ç®—æœŸæœ›è®¡æ•°
            process_missing_record(bn, record, mis_idx[idx], df.columns)

def m_step(bn):
    """
    Mæ­¥ï¼šåŸºäºæœŸæœ›è®¡æ•°é‡æ–°ä¼°è®¡å‚æ•°
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œ
    """
    for node_name in bn.Pres_Graph:
        bn.normalise_cpt(node_name)

def reset_all_counts(bn):
    """é‡ç½®æ‰€æœ‰èŠ‚ç‚¹çš„è®¡æ•°ä¸º0"""
    for node in bn.Pres_Graph.values():
        node.cpt_data['counts'] = 0.0

def process_complete_record(bn, record):
    """
    å¤„ç†å®Œæ•´è®°å½•
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œ
        record: å®Œæ•´çš„è§‚æµ‹è®°å½•
    """
    for node_name, node in bn.Pres_Graph.items():
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = build_query_conditions(node, record)
        
        # æ›´æ–°åŒ¹é…è¡Œçš„è®¡æ•°
        update_node_counts(node, conditions, 1.0)

def process_missing_record(bn, record, missing_var_idx, columns):
    """
    å¤„ç†æœ‰ç¼ºå¤±æ•°æ®çš„è®°å½•
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œ
        record: æœ‰ç¼ºå¤±çš„è§‚æµ‹è®°å½•
        missing_var_idx: ç¼ºå¤±å˜é‡ç´¢å¼•
        columns: DataFrameåˆ—å
    """
    missing_var = columns[missing_var_idx]
    missing_node = bn.search_node(missing_var)
    
    if missing_node is None:
        return
    
    # è®¡ç®—æ¯ä¸ªå¯èƒ½å€¼çš„åéªŒæ¦‚ç‡
    probabilities = compute_posterior_probabilities(bn, record, missing_var, missing_node)
    
    # æŒ‰æ¦‚ç‡æƒé‡æ›´æ–°è®¡æ•°
    for val, prob in enumerate(probabilities):
        if prob > 1e-10:  # å¿½ç•¥æå°æ¦‚ç‡
            complete_record = record.copy()
            complete_record[missing_var] = val
            
            # ä¸ºæ‰€æœ‰èŠ‚ç‚¹æ›´æ–°åŠ æƒè®¡æ•°
            for node_name, node in bn.Pres_Graph.items():
                conditions = build_query_conditions(node, complete_record)
                update_node_counts(node, conditions, prob)

def build_query_conditions(node, record):
    """
    æ„å»ºèŠ‚ç‚¹æŸ¥è¯¢æ¡ä»¶
    
    Args:
        node: å›¾èŠ‚ç‚¹å¯¹è±¡
        record: è§‚æµ‹è®°å½•
    
    Returns:
        dict: æŸ¥è¯¢æ¡ä»¶å­—å…¸
    """
    conditions = {}
    
    # æ·»åŠ çˆ¶èŠ‚ç‚¹å€¼
    for parent in node.Parents:
        if pd.notna(record[parent]):
            conditions[parent] = record[parent]
    
    # æ·»åŠ å½“å‰èŠ‚ç‚¹å€¼
    if pd.notna(record[node.Node_Name]):
        conditions[node.Node_Name] = record[node.Node_Name]
    
    return conditions

def update_node_counts(node, conditions, weight):
    """
    æ›´æ–°èŠ‚ç‚¹è®¡æ•°
    
    Args:
        node: å›¾èŠ‚ç‚¹å¯¹è±¡
        conditions: æŸ¥è¯¢æ¡ä»¶
        weight: æƒé‡
    """
    if not conditions:  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ¡ä»¶ï¼Œè·³è¿‡
        return
        
    # æ„å»ºåŒ¹é…æ©ç 
    mask = pd.Series([True] * len(node.cpt_data))
    for col, val in conditions.items():
        if col in node.cpt_data.columns:
            mask = mask & (node.cpt_data[col] == val)
    
    # æ›´æ–°åŒ¹é…è¡Œçš„è®¡æ•°
    if mask.any():
        node.cpt_data.loc[mask, 'counts'] += weight

def compute_posterior_probabilities(bn, record, missing_var, missing_node):
    """
    è®¡ç®—ç¼ºå¤±å˜é‡çš„åéªŒæ¦‚ç‡åˆ†å¸ƒ
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œ
        record: è§‚æµ‹è®°å½•
        missing_var: ç¼ºå¤±å˜é‡å
        missing_node: ç¼ºå¤±å˜é‡èŠ‚ç‚¹
    
    Returns:
        list: å½’ä¸€åŒ–çš„æ¦‚ç‡åˆ†å¸ƒ
    """
    probabilities = []
    
    # è®¡ç®—æ¯ä¸ªå¯èƒ½å€¼çš„æ¡ä»¶æ¦‚ç‡
    for val in range(missing_node.nvalues):
        prob = compute_conditional_probability(missing_node, record, missing_var, val)
        probabilities.append(prob)
    
    # å½’ä¸€åŒ–
    total_prob = sum(probabilities)
    if total_prob > 0:
        probabilities = [p / total_prob for p in probabilities]
    else:
        # å¦‚æœæ‰€æœ‰æ¦‚ç‡éƒ½æ˜¯0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
        probabilities = [1.0 / missing_node.nvalues] * missing_node.nvalues
    
    return probabilities

def compute_conditional_probability(node, record, var_name, var_value):
    """
    è®¡ç®—ç»™å®šçˆ¶èŠ‚ç‚¹å€¼æ—¶å˜é‡å–ç‰¹å®šå€¼çš„æ¡ä»¶æ¦‚ç‡
    
    Args:
        node: å›¾èŠ‚ç‚¹å¯¹è±¡
        record: è§‚æµ‹è®°å½•
        var_name: å˜é‡å
        var_value: å˜é‡å€¼
    
    Returns:
        float: æ¡ä»¶æ¦‚ç‡
    """
    conditions = {}
    
    # æ·»åŠ çˆ¶èŠ‚ç‚¹æ¡ä»¶
    for parent in node.Parents:
        if pd.notna(record[parent]):
            conditions[parent] = record[parent]
    
    # æ·»åŠ å½“å‰å˜é‡å€¼
    conditions[var_name] = var_value
    
    # åœ¨CPTä¸­æŸ¥æ‰¾æ¦‚ç‡
    mask = pd.Series([True] * len(node.cpt_data))
    for col, val in conditions.items():
        if col in node.cpt_data.columns:
            mask = mask & (node.cpt_data[col] == val)
    
    matched_rows = node.cpt_data[mask]
    if len(matched_rows) > 0 and 'p' in matched_rows.columns:
        return matched_rows['p'].iloc[0]
    else:
        return 1.0 / node.nvalues  # å‡åŒ€å…ˆéªŒ

def compute_log_likelihood(bn, df, mis_idx):
    """
    è®¡ç®—å½“å‰å‚æ•°ä¸‹çš„å¯¹æ•°ä¼¼ç„¶
    
    Args:
        bn: è´å¶æ–¯ç½‘ç»œ
        df: è§‚æµ‹æ•°æ®
        mis_idx: ç¼ºå¤±æ•°æ®ç´¢å¼•
    
    Returns:
        float: å¯¹æ•°ä¼¼ç„¶å€¼
    """
    log_likelihood = 0.0
    
    for idx, (_, record) in enumerate(df.iterrows()):
        if mis_idx[idx] == -1:  # å®Œæ•´è®°å½•
            record_likelihood = compute_record_likelihood(bn, record)
            log_likelihood += np.log(max(record_likelihood, 1e-10))
        else:
            # ç¼ºå¤±è®°å½•çš„è¾¹é™…ä¼¼ç„¶ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            marginal_likelihood = compute_marginal_likelihood(bn, record, mis_idx[idx], df.columns)
            log_likelihood += np.log(max(marginal_likelihood, 1e-10))
    
    return log_likelihood

def compute_record_likelihood(bn, record):
    """è®¡ç®—å®Œæ•´è®°å½•çš„ä¼¼ç„¶"""
    likelihood = 1.0
    
    for node_name, node in bn.Pres_Graph.items():
        conditions = build_query_conditions(node, record)
        
        if len(conditions) == len(node.Parents) + 1:  # æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³
            prob = get_probability_from_cpt(node, conditions)
            likelihood *= max(prob, 1e-10)
    
    return likelihood

def compute_marginal_likelihood(bn, record, missing_var_idx, columns):
    """è®¡ç®—æœ‰ç¼ºå¤±æ•°æ®è®°å½•çš„è¾¹é™…ä¼¼ç„¶"""
    missing_var = columns[missing_var_idx]
    missing_node = bn.search_node(missing_var)
    
    if missing_node is None:
        return 1e-5
    
    marginal_prob = 0.0
    
    # å¯¹ç¼ºå¤±å˜é‡çš„æ¯ä¸ªå¯èƒ½å€¼æ±‚å’Œ
    for val in range(missing_node.nvalues):
        complete_record = record.copy()
        complete_record[missing_var] = val
        
        record_prob = compute_record_likelihood(bn, complete_record)
        marginal_prob += record_prob
    
    return marginal_prob

def get_probability_from_cpt(node, conditions):
    """ä»CPTä¸­è·å–æ¦‚ç‡"""
    mask = pd.Series([True] * len(node.cpt_data))
    for col, val in conditions.items():
        if col in node.cpt_data.columns:
            mask = mask & (node.cpt_data[col] == val)
    
    matched_rows = node.cpt_data[mask]
    if len(matched_rows) > 0 and 'p' in matched_rows.columns:
        return matched_rows['p'].iloc[0]
    else:
        return 1.0 / node.nvalues

def print_convergence_summary(log_likelihood_history):
    """æ‰“å°æ”¶æ•›æ‘˜è¦"""
    print("\nğŸ“ˆ æ”¶æ•›æ‘˜è¦:")
    print(f"   åˆå§‹å¯¹æ•°ä¼¼ç„¶: {log_likelihood_history[0]:.6f}")
    print(f"   æœ€ç»ˆå¯¹æ•°ä¼¼ç„¶: {log_likelihood_history[-1]:.6f}")
    if len(log_likelihood_history) > 1:
        total_improvement = log_likelihood_history[-1] - log_likelihood_history[0]
        print(f"   æ€»æ”¹è¿›: {total_improvement:.6f}")

def save_trained_network(bn, filename):
    """
    ä¿å­˜è®­ç»ƒåçš„ç½‘ç»œ
    
    Args:
        bn: è®­ç»ƒåçš„è´å¶æ–¯ç½‘ç»œ
        filename: ä¿å­˜æ–‡ä»¶å
    """
    try:
        with open(filename, 'wb') as f:
            pickle.dump(bn, f)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def load_trained_network(filename):
    """
    åŠ è½½è®­ç»ƒåçš„ç½‘ç»œ
    
    Args:
        filename: æ–‡ä»¶å
    
    Returns:
        è´å¶æ–¯ç½‘ç»œå¯¹è±¡
    """
    try:
        with open(filename, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None